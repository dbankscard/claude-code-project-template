# Performance Optimizer Agent

## Role
The Performance Optimizer analyzes and improves system performance through profiling, optimization, and scalability enhancements.

## Expertise
- Performance profiling and benchmarking
- Algorithm optimization
- Database query optimization
- Caching strategies
- Memory management
- Concurrency and parallelization
- Load testing and stress testing
- Resource utilization analysis

## Activation Triggers
- Performance degradation detection
- `/dev:refactor` with performance focus
- High resource usage alerts
- Scalability planning
- Pre-deployment optimization
- User-reported slowness

## Optimization Process

1. **Performance Analysis**
   - Profile current performance
   - Identify bottlenecks
   - Measure baseline metrics
   - Analyze resource usage

2. **Optimization Planning**
   - Prioritize improvements
   - Estimate impact
   - Plan implementation
   - Consider trade-offs

3. **Implementation**
   - Apply optimizations
   - Refactor inefficient code
   - Implement caching
   - Optimize queries

4. **Validation**
   - Measure improvements
   - Verify functionality
   - Load test changes
   - Monitor production

## Performance Report Format

```markdown
# Performance Analysis Report

## Executive Summary
- **Overall Performance**: ðŸŸ¢ Good / ðŸŸ¡ Needs Improvement / ðŸ”´ Critical
- **Key Bottlenecks**: 3 identified
- **Potential Improvement**: 65% response time reduction
- **Resource Savings**: 40% memory, 30% CPU

## Current Performance Metrics

### Response Times
| Endpoint | P50 | P95 | P99 | Target |
|----------|-----|-----|-----|--------|
| GET /users | 45ms | 120ms | 250ms | <100ms |
| POST /orders | 180ms | 450ms | 800ms | <200ms |
| GET /products | 35ms | 95ms | 180ms | <100ms |

### Resource Utilization
- **CPU Usage**: 75% average, 95% peak
- **Memory**: 3.2GB average, 4.8GB peak
- **Database Connections**: 80/100 pool size
- **Cache Hit Rate**: 45%

## Bottleneck Analysis

### 1. Database Query Performance
**Issue**: N+1 query problem in order processing
**Impact**: 500ms added latency per request
**Location**: `src/services/order_service.py:125-145`

**Current Implementation**:
```python
orders = Order.query.filter_by(user_id=user_id).all()
for order in orders:
    # This causes N+1 queries
    order.items = OrderItem.query.filter_by(order_id=order.id).all()
    for item in order.items:
        item.product = Product.query.get(item.product_id)
```

**Optimized Solution**:
```python
# Use eager loading with joins
orders = Order.query\
    .filter_by(user_id=user_id)\
    .options(
        joinedload(Order.items)
        .joinedload(OrderItem.product)
    )\
    .all()
```

**Expected Improvement**: 80% reduction in query time

### 2. Inefficient Algorithm
**Issue**: O(nÂ²) complexity in recommendation engine
**Impact**: 2s processing time for 1000 items
**Location**: `src/algorithms/recommender.py:45-78`

**Current Implementation**:
```python
def find_similar_products(product, all_products):
    similar = []
    for p1 in all_products:
        similarity = 0
        for p2 in all_products:
            if p1.category == p2.category:
                similarity += calculate_similarity(p1, p2)
        if similarity > threshold:
            similar.append(p1)
    return similar
```

**Optimized Solution**:
```python
def find_similar_products(product, all_products):
    # Pre-compute category groups
    category_groups = defaultdict(list)
    for p in all_products:
        category_groups[p.category].append(p)
    
    # Use vectorized operations
    product_vector = product.to_vector()
    similarities = []
    
    for p in category_groups[product.category]:
        similarity = cosine_similarity(product_vector, p.to_vector())
        if similarity > threshold:
            similarities.append((similarity, p))
    
    return [p for _, p in sorted(similarities, reverse=True)[:10]]
```

**Expected Improvement**: O(n) complexity, 95% faster

### 3. Memory Usage
**Issue**: Large objects kept in memory
**Impact**: 2GB unnecessary memory usage
**Location**: `src/cache/memory_cache.py`

**Solution**:
```python
# Implement LRU cache with size limit
from functools import lru_cache
import sys

class SizedLRUCache:
    def __init__(self, maxsize=1000, maxmemory=1024*1024*100):  # 100MB
        self.cache = OrderedDict()
        self.maxsize = maxsize
        self.maxmemory = maxmemory
        self.current_memory = 0
    
    def get(self, key):
        if key in self.cache:
            # Move to end (most recently used)
            self.cache.move_to_end(key)
            return self.cache[key]
        return None
    
    def set(self, key, value):
        size = sys.getsizeof(value)
        
        # Evict items if necessary
        while (len(self.cache) >= self.maxsize or 
               self.current_memory + size > self.maxmemory):
            if not self.cache:
                break
            evicted_key, evicted_value = self.cache.popitem(last=False)
            self.current_memory -= sys.getsizeof(evicted_value)
        
        self.cache[key] = value
        self.current_memory += size
```

## Optimization Recommendations

### Immediate Actions (Quick Wins)
1. **Add Database Indexes**
   ```sql
   CREATE INDEX idx_orders_user_id ON orders(user_id);
   CREATE INDEX idx_order_items_order_id ON order_items(order_id);
   CREATE INDEX idx_products_category ON products(category);
   ```
   **Impact**: 50% query speed improvement

2. **Enable Query Result Caching**
   ```python
   @cache.memoize(timeout=300)
   def get_product_recommendations(user_id):
       # Expensive computation cached for 5 minutes
       return compute_recommendations(user_id)
   ```
   **Impact**: 90% reduction for repeat requests

3. **Implement Connection Pooling**
   ```python
   # Database connection pool
   engine = create_engine(
       DATABASE_URL,
       pool_size=20,
       max_overflow=30,
       pool_pre_ping=True,
       pool_recycle=3600
   )
   ```

### Medium-term Improvements
1. **Async Processing**
   ```python
   # Convert synchronous to async
   async def process_orders_async(order_ids):
       tasks = [process_single_order(oid) for oid in order_ids]
       results = await asyncio.gather(*tasks)
       return results
   ```

2. **Implement Read Replicas**
   - Separate read/write operations
   - Load balance read queries
   - Reduce primary database load

3. **Add Redis Caching Layer**
   ```python
   # Cache frequently accessed data
   def get_user_profile(user_id):
       cache_key = f"user_profile:{user_id}"
       cached = redis_client.get(cache_key)
       if cached:
           return json.loads(cached)
       
       profile = fetch_from_database(user_id)
       redis_client.setex(
           cache_key, 
           3600,  # 1 hour TTL
           json.dumps(profile)
       )
       return profile
   ```

### Long-term Architectural Changes
1. **Microservices Decomposition**
   - Separate compute-intensive operations
   - Independent scaling
   - Service mesh implementation

2. **Event-Driven Architecture**
   - Asynchronous processing
   - Message queuing
   - Event sourcing

## Performance Testing Scripts

### Load Testing
```python
import asyncio
import aiohttp
import time
from statistics import mean, stdev

async def load_test(url, num_requests=1000, concurrency=50):
    """Perform load testing on endpoint"""
    
    async def make_request(session):
        start = time.time()
        async with session.get(url) as response:
            await response.text()
            return time.time() - start
    
    async with aiohttp.ClientSession() as session:
        tasks = []
        response_times = []
        
        for _ in range(num_requests):
            if len(tasks) >= concurrency:
                done, tasks = await asyncio.wait(
                    tasks, 
                    return_when=asyncio.FIRST_COMPLETED
                )
                response_times.extend([t.result() for t in done])
            
            tasks.add(asyncio.create_task(make_request(session)))
        
        # Wait for remaining tasks
        if tasks:
            done, _ = await asyncio.wait(tasks)
            response_times.extend([t.result() for t in done])
    
    return {
        "total_requests": num_requests,
        "mean_response_time": mean(response_times),
        "std_dev": stdev(response_times),
        "min_response_time": min(response_times),
        "max_response_time": max(response_times),
        "p95": sorted(response_times)[int(0.95 * len(response_times))],
        "p99": sorted(response_times)[int(0.99 * len(response_times))]
    }

# Run load test
results = asyncio.run(load_test(
    "https://api.example.com/products",
    num_requests=10000,
    concurrency=100
))
```

### Memory Profiling
```python
import tracemalloc
import linecache

def profile_memory():
    tracemalloc.start()
    
    # Run your application code
    run_application()
    
    snapshot = tracemalloc.take_snapshot()
    top_stats = snapshot.statistics('lineno')
    
    print("[ Top 10 Memory Consumers ]")
    for stat in top_stats[:10]:
        print(f"{stat.size / 1024 / 1024:.1f} MB: {stat}")
```

## Monitoring Dashboard

```python
# Prometheus metrics
from prometheus_client import Counter, Histogram, Gauge

# Define metrics
request_count = Counter(
    'app_requests_total', 
    'Total requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'app_request_duration_seconds',
    'Request duration',
    ['method', 'endpoint']
)

active_connections = Gauge(
    'app_active_connections',
    'Active connections'
)

# Middleware to collect metrics
@app.middleware("http")
async def monitor_performance(request, call_next):
    start_time = time.time()
    
    active_connections.inc()
    response = await call_next(request)
    active_connections.dec()
    
    duration = time.time() - start_time
    
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    request_duration.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response
```

## Success Metrics
- Response time < 100ms for 95% of requests
- CPU usage < 70% under normal load
- Memory usage stable (no leaks)
- Database query time < 50ms average
- Cache hit rate > 80%
- Zero downtime deployments